{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2047221,"sourceType":"datasetVersion","datasetId":1226038}],"dockerImageVersionId":30154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ML Classification Into SQL\n- Build Classification Model\n- Extract features and build SQL Code Implementation\n- SQL Code Logistic Regression Implementation:\n\n# Key Features:\n### Data Loading and Preprocessing:\n\n- The script loads a heart-related dataset from a CSV file, handles missing values, and encodes categorical variables.\n- Robust scaling is applied to continuous features.\n\n### Model Training and Evaluation:\n\n- Several classification models (e.g., AdaBoost, Logistic Regression, Random Forest, Gradient Boosting) are trained and evaluated.\n- Model evaluation includes accuracy, confusion matrix, and ROC AUC. Cross-validation is implemented for more robust performance assessment.\n\n### Hyperparameter Tuning:\n\n- Hyperparameter tuning is performed for RandomForestClassifier and LogisticRegression using GridSearchCV.\n\n### Feature Importance (Tree-based Models):\n\n- For tree-based models, feature importance is extracted and displayed.\n\n### Model Saving:\n\n- Trained models are saved using joblib for potential deployment.\n","metadata":{}},{"cell_type":"code","source":"try:\n    import imblearn\nexcept ImportError:\n    print(\"imbalanced-learn not found. Installing...\")\n    !pip install imbalanced-learn\n    print(\"imbalanced-learn installed successfully!\")\n\n# Now import and use imbalanced-learn\nfrom imblearn.over_sampling import RandomOverSampler","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-03-27T12:38:20.134354Z","iopub.execute_input":"2024-03-27T12:38:20.135086Z","iopub.status.idle":"2024-03-27T12:38:22.046795Z","shell.execute_reply.started":"2024-03-27T12:38:20.135053Z","shell.execute_reply":"2024-03-27T12:38:22.045615Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Importing necessary packages\nimport os\nimport numpy as np\nimport pandas as pd\nimport logging\nimport warnings\nimport joblib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport csv\n\nfrom sklearn.calibration import calibration_curve\nfrom scikitplot.metrics import plot_precision_recall\nfrom sklearn.metrics import precision_recall_curve, auc\nfrom scikitplot.metrics import plot_lift_curve\nfrom scikitplot.metrics import plot_cumulative_gain\nfrom sklearn.metrics import classification_report, roc_curve, accuracy_score, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score, precision_recall_curve\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier,ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.exceptions import ConvergenceWarning\n\nimport xgboost as xgb\nimport sqlite3\n\n\n# Setting up options and ignoring warnings\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\n# Setting up logging with a FileHandler\nlog_file_path = 'classification_log.txt'\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n\nfile_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel(logging.INFO)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(file_handler)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-03-27T12:38:22.048926Z","iopub.execute_input":"2024-03-27T12:38:22.049487Z","iopub.status.idle":"2024-03-27T12:38:24.323584Z","shell.execute_reply.started":"2024-03-27T12:38:22.049457Z","shell.execute_reply":"2024-03-27T12:38:24.322370Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Reading the dataset\ndataset_path = \"/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv\"\n/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv\ntry:\n    # Attempt to read the dataset\n    df = pd.read_csv(dataset_path)\n    logger.info(f\"Dataset loaded successfully from {dataset_path}\")\nexcept FileNotFoundError:\n    logger.error(\"Error: Dataset file not found. Please provide the correct file path.\")\nexcept Exception as e:\n    logger.error(f\"An error occurred: {e}\")\n    \ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T12:38:24.324881Z","iopub.execute_input":"2024-03-27T12:38:24.325434Z","iopub.status.idle":"2024-03-27T12:38:24.962474Z","shell.execute_reply.started":"2024-03-27T12:38:24.325404Z","shell.execute_reply":"2024-03-27T12:38:24.960405Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     11\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"],"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# creating a copy of df\ndf1 = df.fillna(0)\n\n# define the columns to be encoded and scaled\ncat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\n#target column name\nlabel_name = 'output'\n\n# encoding the categorical columns\ndf1 = pd.get_dummies(df1, columns = cat_cols, drop_first = True)\n\n# defining the features and target\nX = df1.drop([label_name],axis=1)\ny = df1[[label_name]]\n\n# instantiating the scaler\nscaler = RobustScaler()\n\n# scaling the continuous featuree\nX[con_cols] = scaler.fit_transform(X[con_cols])\n\n# Get centering and scaling values for each feature\ncentering = scaler.center_\niqrs  = scaler.scale_\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T12:38:24.963643Z","iopub.status.idle":"2024-03-27T12:38:24.964525Z","shell.execute_reply.started":"2024-03-27T12:38:24.964257Z","shell.execute_reply":"2024-03-27T12:38:24.964276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\nprint(\"The shape of X_train is      \", X_train.shape)\nprint(\"The shape of X_test is       \",X_test.shape)\nprint(\"The shape of y_train is      \",y_train.shape)\nprint(\"The shape of y_test is       \",y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T12:38:24.968230Z","iopub.status.idle":"2024-03-27T12:38:24.969569Z","shell.execute_reply.started":"2024-03-27T12:38:24.969191Z","shell.execute_reply":"2024-03-27T12:38:24.969221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handling Class Imbalance with RandomOverSampler\nros = RandomOverSampler(sampling_strategy=0.9, random_state=42)\nX_resampled, y_resampled = ros.fit_resample(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T12:38:24.970977Z","iopub.status.idle":"2024-03-27T12:38:24.971390Z","shell.execute_reply.started":"2024-03-27T12:38:24.971189Z","shell.execute_reply":"2024-03-27T12:38:24.971205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The shape of X_train is      \", X_resampled.shape)\nprint(\"The shape of y_train is      \", y_resampled.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T12:38:24.973631Z","iopub.status.idle":"2024-03-27T12:38:24.974251Z","shell.execute_reply.started":"2024-03-27T12:38:24.973957Z","shell.execute_reply":"2024-03-27T12:38:24.973983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### For binary classification:\n- accuracy: Calculates the accuracy of the classifier.\n- precision': Measures the ability of the classifier not to label as positive a sample that is negative.\n- recall: Measures the ability of the classifier to capture all the positive samples.\n- f1: Combines precision and recall into a single metric.\n- roc_auc: Computes the area under the Receiver Operating Characteristic (ROC) curve.","metadata":{}},{"cell_type":"code","source":"df_results = pd.DataFrame(model_results)\ndf_results[['Model','Accuracy','ROC AUC','Precision','Recall','F1 Score']]","metadata":{"execution":{"iopub.status.busy":"2024-03-27T12:38:24.976177Z","iopub.status.idle":"2024-03-27T12:38:24.976732Z","shell.execute_reply.started":"2024-03-27T12:38:24.976459Z","shell.execute_reply":"2024-03-27T12:38:24.976482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning RandomForest","metadata":{}},{"cell_type":"code","source":"# instantiating the object\nmodel = RandomForestClassifier()\n\n# setting a grid - not so extensive\nparameters = {'n_estimators': [50, 60],\n    'max_depth': [2,3,10,15,20,25, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]}\n\nsearcher = GridSearchCV(estimator = model, param_grid = parameters,cv=5, scoring='accuracy')\n\n# fitting the object\nsearcher.fit(X_train, y_train)\n\n# the scores\nprint(\"The best params are :\", searcher.best_params_)\nprint(\"The best score is   :\", searcher.best_score_)\n\n# predicting the values\ny_pred = searcher.predict(X_test)\n\n# printing the test accuracy\nprint(\"The test accuracy score of model after hyper-parameter tuning is \", accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-03-27T12:38:24.978903Z","iopub.status.idle":"2024-03-27T12:38:24.979959Z","shell.execute_reply.started":"2024-03-27T12:38:24.979693Z","shell.execute_reply":"2024-03-27T12:38:24.979715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a pivot table of mean test scores for each parameter combination\npivot_table = results_df.pivot_table(index='param_max_iter', columns='param_C', values='mean_test_score')\n\n# Plot the heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(pivot_table, annot=True, cmap='YlGnBu', fmt=\".4f\", linewidths=0.5)\nplt.title('Mean Test Scores for Different max_iter and C Values')\nplt.xlabel('C (Regularization Parameter)')\nplt.ylabel('max_iter (Maximum Number of Iterations)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T12:38:24.981840Z","iopub.status.idle":"2024-03-27T12:38:24.982250Z","shell.execute_reply.started":"2024-03-27T12:38:24.982046Z","shell.execute_reply":"2024-03-27T12:38:24.982061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select subset of parameters for pairwise interaction plots\nparam_subset = ['param_max_iter', 'param_C', 'param_tol','mean_test_score']\n\n# Create pairplot\nsns.pairplot(results_df, hue='param_solver', vars=param_subset, diag_kind='kde')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T12:38:24.983555Z","iopub.status.idle":"2024-03-27T12:38:24.983920Z","shell.execute_reply.started":"2024-03-27T12:38:24.983731Z","shell.execute_reply":"2024-03-27T12:38:24.983745Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2047221,"sourceType":"datasetVersion","datasetId":1226038}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ML Classification Into SQL\n- Build Classification Model\n- Extract features and build SQL Code Implementation\n- SQL Code Logistic Regression Implementation:\n\n# Key Features:\n### Data Loading and Preprocessing:\n\n- The script loads a heart-related dataset from a CSV file, handles missing values, and encodes categorical variables.\n- Robust scaling is applied to continuous features.\n\n### Model Training and Evaluation:\n\n- Several classification models (e.g., AdaBoost, Logistic Regression, Random Forest, Gradient Boosting) are trained and evaluated.\n- Model evaluation includes accuracy, confusion matrix, and ROC AUC. Cross-validation is implemented for more robust performance assessment.\n\n### Hyperparameter Tuning:\n\n- Hyperparameter tuning is performed for RandomForestClassifier and LogisticRegression using GridSearchCV.\n\n### Feature Importance (Tree-based Models):\n\n- For tree-based models, feature importance is extracted and displayed.\n\n### Model Saving:\n\n- Trained models are saved using joblib for potential deployment.\n","metadata":{}},{"cell_type":"code","source":"try:\n    import imblearn\nexcept ImportError:\n    print(\"imbalanced-learn not found. Installing...\")\n    !pip install imbalanced-learn\n    print(\"imbalanced-learn installed successfully!\")\n\n# Now import and use imbalanced-learn\nfrom imblearn.over_sampling import RandomOverSampler","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-27T13:07:03.213395Z","iopub.execute_input":"2024-03-27T13:07:03.213841Z","iopub.status.idle":"2024-03-27T13:07:05.328409Z","shell.execute_reply.started":"2024-03-27T13:07:03.213805Z","shell.execute_reply":"2024-03-27T13:07:05.327195Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Importing necessary packages\nimport os\nimport numpy as np\nimport pandas as pd\nimport logging\nimport warnings\nimport joblib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport csv\n\nfrom sklearn.calibration import calibration_curve\nfrom scikitplot.metrics import plot_precision_recall\nfrom sklearn.metrics import precision_recall_curve, auc\nfrom scikitplot.metrics import plot_lift_curve\nfrom scikitplot.metrics import plot_cumulative_gain\nfrom sklearn.metrics import classification_report, roc_curve, accuracy_score, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score, precision_recall_curve\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier,ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.exceptions import ConvergenceWarning\n\nimport xgboost as xgb\nimport sqlite3\n\n\n# Setting up options and ignoring warnings\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\n# Setting up logging with a FileHandler\nlog_file_path = 'classification_log.txt'\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n\nfile_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel(logging.INFO)\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(file_handler)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-27T13:07:05.330875Z","iopub.execute_input":"2024-03-27T13:07:05.331382Z","iopub.status.idle":"2024-03-27T13:07:07.918289Z","shell.execute_reply.started":"2024-03-27T13:07:05.331348Z","shell.execute_reply":"2024-03-27T13:07:07.917330Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Reading the dataset\ndataset_path = \"../kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv\"\n\n# Reading the dataset\n#dataset_path = \"../input/heart-attack-analysis-prediction-dataset/heart.csv\"\n#/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv\n\ntry:\n    # Attempt to read the dataset\n    df = pd.read_csv(dataset_path)\n    logger.info(f\"Dataset loaded successfully from {dataset_path}\")\nexcept FileNotFoundError:\n    logger.error(\"Error: Dataset file not found. Please provide the correct file path.\")\nexcept Exception as e:\n    logger.error(f\"An error occurred: {e}\")\n    \ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T13:07:07.919559Z","iopub.execute_input":"2024-03-27T13:07:07.920077Z","iopub.status.idle":"2024-03-27T13:07:08.576230Z","shell.execute_reply.started":"2024-03-27T13:07:07.920046Z","shell.execute_reply":"2024-03-27T13:07:08.573307Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     15\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"],"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# creating a copy of df\ndf1 = df.fillna(0)\n\n# define the columns to be encoded and scaled\ncat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\n#target column name\nlabel_name = 'output'\n\n# encoding the categorical columns\ndf1 = pd.get_dummies(df1, columns = cat_cols, drop_first = True)\n\n# defining the features and target\nX = df1.drop([label_name],axis=1)\ny = df1[[label_name]]\n\n# instantiating the scaler\nscaler = RobustScaler()\n\n# scaling the continuous featuree\nX[con_cols] = scaler.fit_transform(X[con_cols])\n\n# Get centering and scaling values for each feature\ncentering = scaler.center_\niqrs  = scaler.scale_\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T13:07:08.577088Z","iopub.status.idle":"2024-03-27T13:07:08.577560Z","shell.execute_reply.started":"2024-03-27T13:07:08.577343Z","shell.execute_reply":"2024-03-27T13:07:08.577362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\nprint(\"The shape of X_train is      \", X_train.shape)\nprint(\"The shape of X_test is       \",X_test.shape)\nprint(\"The shape of y_train is      \",y_train.shape)\nprint(\"The shape of y_test is       \",y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T13:07:08.578870Z","iopub.status.idle":"2024-03-27T13:07:08.579303Z","shell.execute_reply.started":"2024-03-27T13:07:08.579081Z","shell.execute_reply":"2024-03-27T13:07:08.579097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handling Class Imbalance with RandomOverSampler\nros = RandomOverSampler(sampling_strategy=0.9, random_state=42)\nX_resampled, y_resampled = ros.fit_resample(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T13:07:08.580443Z","iopub.status.idle":"2024-03-27T13:07:08.580832Z","shell.execute_reply.started":"2024-03-27T13:07:08.580642Z","shell.execute_reply":"2024-03-27T13:07:08.580657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The shape of X_train is      \", X_resampled.shape)\nprint(\"The shape of y_train is      \", y_resampled.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T13:07:08.582159Z","iopub.status.idle":"2024-03-27T13:07:08.582713Z","shell.execute_reply.started":"2024-03-27T13:07:08.582456Z","shell.execute_reply":"2024-03-27T13:07:08.582479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### For binary classification:\n- accuracy: Calculates the accuracy of the classifier.\n- precision': Measures the ability of the classifier not to label as positive a sample that is negative.\n- recall: Measures the ability of the classifier to capture all the positive samples.\n- f1: Combines precision and recall into a single metric.\n- roc_auc: Computes the area under the Receiver Operating Characteristic (ROC) curve.","metadata":{}},{"cell_type":"code","source":"df_results = pd.DataFrame(model_results)\ndf_results[['Model','Accuracy','ROC AUC','Precision','Recall','F1 Score']]","metadata":{"execution":{"iopub.status.busy":"2024-03-27T13:07:08.583950Z","iopub.status.idle":"2024-03-27T13:07:08.585092Z","shell.execute_reply.started":"2024-03-27T13:07:08.584781Z","shell.execute_reply":"2024-03-27T13:07:08.584804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning RandomForest","metadata":{}},{"cell_type":"code","source":"%%time\n\n# instantiating the object\nmodel = RandomForestClassifier()\n\n# setting a grid - not so extensive\nparameters = {'n_estimators': [300,400],\n    'max_depth': [24],\n    'min_samples_split': [2,3],\n    'min_samples_leaf': [7,8],\n    'verbose': [0],  # Verbosity\n    'criterion': ['gini','entropy'],\n    'max_features': ['auto','sqrt','log2'],\n    'bootstrap':[True,False],\n    'class_weight':[None,'balanced','balanced_subsample'],\n    'warm_start': [True],  # Warm start\n    'random_state': [42],  # Random state for reproducibility         \n    'oob_score':[True,False],\n    'n_jobs': [-1]}\n\nsearcher = GridSearchCV(estimator = model, param_grid = parameters,cv=2, scoring='roc_auc',verbose=2, n_jobs=-1)\n\n# fitting the object\nsearcher.fit(X_train, y_train)\n\n# the scores\nprint(\"The best params are :\", searcher.best_params_)\nprint(\"The best score is   :\", searcher.best_score_)\n\n# predicting the values\ny_pred = searcher.predict(X_test)\n\n# printing the test accuracy\nprint(\"The test accuracy score of model after hyper-parameter tuning is \", accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-03-27T13:07:08.588396Z","iopub.status.idle":"2024-03-27T13:07:08.589465Z","shell.execute_reply.started":"2024-03-27T13:07:08.589134Z","shell.execute_reply":"2024-03-27T13:07:08.589160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the best hyperparameters from the GridSearchCV\nbest_params = searcher.best_params_\nprint(best_params)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T13:07:08.590937Z","iopub.status.idle":"2024-03-27T13:07:08.591523Z","shell.execute_reply.started":"2024-03-27T13:07:08.591246Z","shell.execute_reply":"2024-03-27T13:07:08.591270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=RandomForestClassifier(**best_params)\nmodel.fit(X_resampled, y_resampled.values.ravel())\npredict = model.predict(X_test)\nacc = accuracy_score(y_test, predict)\nconf = confusion_matrix(y_test, predict)\n\ny_pred_proba = model.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nroc_auc = roc_auc_score(y_test,y_pred_proba)\n\nprint(acc)\nprint(conf)    \nprint(classification_report(y_test,predict))","metadata":{"execution":{"iopub.status.busy":"2024-03-27T13:07:08.593832Z","iopub.status.idle":"2024-03-27T13:07:08.594687Z","shell.execute_reply.started":"2024-03-27T13:07:08.594404Z","shell.execute_reply":"2024-03-27T13:07:08.594429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Confusion Matrix\nplt.figure(figsize=(2,2))\nsns.heatmap(conf, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()\n\n# Plot ROC Curve\nplt.figure(figsize=(8, 8))\nplt.plot(fpr, tpr, color='black', lw=2, label='ROC curve (area = {:.4f})'.format(roc_auc))\nplt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T13:07:08.596090Z","iopub.status.idle":"2024-03-27T13:07:08.597156Z","shell.execute_reply.started":"2024-03-27T13:07:08.596933Z","shell.execute_reply":"2024-03-27T13:07:08.596953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importances = model.feature_importances_\nfeature_names = X_train.columns\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\nsorted_df = importance_df.sort_values(by='Importance', ascending=False)\nsorted_df = sorted_df.head(10)\n\n# Print sorted feature importance\nprint(f\" - Feature Importance:\")\nprint(importance_df.sort_values(by='Importance', ascending=False))\n\n # Create seaborn bar plot\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=sorted_df, palette='viridis')\nplt.title(f\"- Feature Importance\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T13:07:08.598654Z","iopub.status.idle":"2024-03-27T13:07:08.599038Z","shell.execute_reply.started":"2024-03-27T13:07:08.598856Z","shell.execute_reply":"2024-03-27T13:07:08.598871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import Pipeline\nimport joblib\n\n# Load the iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the preprocessing pipeline with RobustScaler\npreprocessor = Pipeline(steps=[('scaler', RobustScaler())])\n\n# Define the Random Forest classifier\nrf_clf = RandomForestClassifier()\n\n# Define the hyperparameters grid\nparam_grid = {\n    'classifier__n_estimators': [50, 100, 150],\n    'classifier__criterion': ['gini', 'entropy'],\n    'classifier__max_depth': [None, 10, 20],\n    'classifier__min_samples_split': [2, 5, 10],\n    'classifier__min_samples_leaf': [1, 2, 4],\n    'classifier__max_features': ['auto', 'sqrt', 'log2'],\n    'classifier__bootstrap': [True, False],\n    'classifier__class_weight': [None, 'balanced', 'balanced_subsample'],\n    'classifier__random_state': [42],\n    'classifier__warm_start': [True, False],\n    'classifier__oob_score': [True, False]\n}\n\n# Define GridSearchCV with preprocessing pipeline\ngrid_search = GridSearchCV(estimator=Pipeline([('preprocessor', preprocessor), ('classifier', rf_clf)]),\n                           param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n\n# Fit GridSearchCV\ngrid_search.fit(X_train, y_train)\n\n# Save the model to disk\nsaved_model_path = 'saved_model.pkl'\njoblib.dump(grid_search.best_estimator_, saved_model_path)\n\n# Load the saved model\nloaded_model = joblib.load(saved_model_path)\n\n# Load the new data\nnew_data = np.array([[6.1, 2.8, 4.7, 1.2], [5.5, 2.6, 4.4, 1.5], [7.3, 2.9, 6.3, 1.8]])\n\n# Preprocess the new data using the saved preprocessing pipeline\npreprocessed_data = loaded_model.named_steps['preprocessor'].transform(new_data)\n\n# Make predictions using the loaded model\npredictions = loaded_model.predict(preprocessed_data)\n\n# Get probability scores for each class\nprobability_scores = loaded_model.predict_proba(preprocessed_data)\n\n# Print or use the predictions and probability scores as needed\nprint(\"Predictions for new data:\")\nprint(predictions)\nprint(\"Probability scores for each class:\")\nprint(probability_scores)\n","metadata":{},"execution_count":null,"outputs":[]}]}
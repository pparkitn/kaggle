{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7624364,"sourceType":"datasetVersion","datasetId":4441201},{"sourceId":4295427,"sourceType":"datasetVersion","datasetId":2529204}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dsptlp/spark?scriptVersionId=163495518\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# SPARK \n- Reasons to Use Spark\n- This notebook will compare Spark VS Pandas\n\n# NOTE \n- Spark is designed to work in a distributed computing environment and is most effective when dealing with large datasets and clusters of machines. \n- In Kaggle's limited environment, we are not using a distributed computing environment but will be able to use all the computer resources which will be the only benefit. \n\n# SPARK ADVANTAGES\n\n1. **Speed:** Spark is known for its speed, as it can perform in-memory processing, reducing the need to write intermediate results to disk. This makes Spark well-suited for iterative algorithms and interactive data analysis.\n\n2. **Ease of Use:** Spark provides high-level APIs in languages such as Scala, Java, Python, and R, making it accessible to a wide range of users. It also offers built-in libraries for various tasks like SQL, machine learning (MLlib), graph processing (GraphX), and stream processing (Spark Streaming).\n\n3. **Scalability:** Spark is designed for distributed computing, allowing it to scale horizontally across a cluster of machines. This makes it suitable for handling large datasets and processing tasks that would be challenging for single-node systems.\n\n4. **Versatility:** Spark supports a variety of data processing scenarios, including batch processing, interactive queries, streaming analytics, and machine learning. This versatility makes it a preferred choice for organizations with diverse data analysis needs.\n\n5. **Fault Tolerance:** Spark provides fault tolerance through lineage information and resilient distributed datasets (RDDs). If a node fails, Spark can recompute the lost data using the lineage information, ensuring the reliability of data processing.\n\n6. **Integration with Big Data Ecosystem:** Spark seamlessly integrates with other big data tools and technologies, such as Hadoop Distributed File System (HDFS), Apache Hive, Apache HBase, and more. This allows users to leverage existing data storage and processing systems.\n\n7. **Community Support:** Spark has a large and active open-source community. This means continuous development, improvements, and a wealth of resources, including documentation, forums, and tutorials.\n\n8. **In-Memory Processing:** Spark's ability to store intermediate data in memory rather than writing to disk can significantly improve performance, especially for iterative algorithms and interactive data analysis, compared to traditional disk-based processing.\n","metadata":{}},{"cell_type":"code","source":"# Install PySpark\ntry:\n    import pyspark\nexcept ImportError:\n    print(\"pyspark not found. Installing...\")\n    !pip install pyspark > pyspark.log.txt\n    print(\"pyspark installed successfully!\")","metadata":{"execution":{"iopub.status.busy":"2024-02-20T00:38:01.340874Z","iopub.execute_input":"2024-02-20T00:38:01.341571Z","iopub.status.idle":"2024-02-20T00:38:01.348582Z","shell.execute_reply.started":"2024-02-20T00:38:01.341516Z","shell.execute_reply":"2024-02-20T00:38:01.34771Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nfrom pyspark.sql import SparkSession\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom matplotlib.lines import Line2D\nfrom matplotlib import cm\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport warnings\nimport timeit\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\n# Suppress all warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-02-20T00:38:01.35017Z","iopub.execute_input":"2024-02-20T00:38:01.351116Z","iopub.status.idle":"2024-02-20T00:38:01.362237Z","shell.execute_reply.started":"2024-02-20T00:38:01.351037Z","shell.execute_reply":"2024-02-20T00:38:01.360915Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Create a Spark session\nspark = SparkSession.builder.appName(\"Spark\").getOrCreate()\n\n# Set log level to OFF \nspark.sparkContext.setLogLevel(\"OFF\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-20T00:38:01.364636Z","iopub.execute_input":"2024-02-20T00:38:01.364988Z","iopub.status.idle":"2024-02-20T00:38:02.613892Z","shell.execute_reply.started":"2024-02-20T00:38:01.36496Z","shell.execute_reply":"2024-02-20T00:38:02.612829Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"file_path  = \"/kaggle/input/tabular-dataset-ready-for-malicious-url-detection/train_dataset.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-02-20T00:38:02.616399Z","iopub.execute_input":"2024-02-20T00:38:02.616729Z","iopub.status.idle":"2024-02-20T00:38:02.622295Z","shell.execute_reply.started":"2024-02-20T00:38:02.616702Z","shell.execute_reply":"2024-02-20T00:38:02.621193Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"measures = []","metadata":{"execution":{"iopub.status.busy":"2024-02-20T00:38:02.62456Z","iopub.execute_input":"2024-02-20T00:38:02.625416Z","iopub.status.idle":"2024-02-20T00:38:02.634841Z","shell.execute_reply.started":"2024-02-20T00:38:02.625377Z","shell.execute_reply":"2024-02-20T00:38:02.633517Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# LOADING DATA","metadata":{}},{"cell_type":"markdown","source":"## SPARK","metadata":{}},{"cell_type":"code","source":"%%time\n\ndef load_csv_using_spark():\n    df = spark.read.csv(file_path, header=True, inferSchema=True)\n\n    # Perform the summary: count number of records grouped by a column\n    summary_df = df.groupBy(\"label\").count()\n    \n    # Perform an action (triggers execution, note that spark uses Lazy Execution)\n    summary_df.collect() #show()\n    \n# Measure the execution time\nexecution_time = timeit.timeit(load_csv_using_spark, number=10)\n\n# Print the result\nprint(f\"Execution time using SPARK: {execution_time} seconds\")\nmeasures.append(('SPARK','load_csv',execution_time))","metadata":{"execution":{"iopub.status.busy":"2024-02-20T00:38:08.846251Z","iopub.execute_input":"2024-02-20T00:38:08.847179Z","iopub.status.idle":"2024-02-20T00:48:12.692975Z","shell.execute_reply.started":"2024-02-20T00:38:08.847142Z","shell.execute_reply":"2024-02-20T00:48:12.691578Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"[Stage 47:================================================>       (13 + 2) / 15]\r","output_type":"stream"},{"name":"stdout","text":"Execution time using SPARK: 603.830106264 seconds\nCPU times: user 386 ms, sys: 92.4 ms, total: 478 ms\nWall time: 10min 3s\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"## PANDAS","metadata":{}},{"cell_type":"code","source":"%%time\n\ndef load_csv_using_pandas():\n    data_df = pd.read_csv(file_path, delimiter=',') \n    summary_df = data_df[['url_has_login','label']].groupby(['label']).count()\n    \n# Measure the execution time\nexecution_time = timeit.timeit(load_csv_using_pandas, number=10)\n\n# Print the result\nprint(f\"Execution time using PANDAS: {execution_time} seconds\")\nmeasures.append(('PANDAS','load_csv',execution_time))","metadata":{"execution":{"iopub.status.busy":"2024-02-20T00:49:52.873635Z","iopub.execute_input":"2024-02-20T00:49:52.874011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FORMAT DATA","metadata":{}},{"cell_type":"markdown","source":"## SPARK","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PANDAS","metadata":{}},{"cell_type":"code","source":"data_subset_df = data_df[['label','url_has_login','url_has_client','url_has_server','url_len']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RANDOMFOREST","metadata":{}},{"cell_type":"markdown","source":"## SPARK","metadata":{}},{"cell_type":"code","source":"# Example: Assuming you have a binary classification problem\nassembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", ...], outputCol=\"features\")\ndf = assembler.transform(df)\n\n# Split the data into training and testing sets\ntrain_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n\npipeline = Pipeline(stages=[assembler, rf])\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(rf.numTrees, [10, 20, 30])\n             .addGrid(rf.maxDepth, [5, 10, 15])\n             .build())\n\nevaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=5)\n\ncv_model = crossval.fit(train_data)\n\npredictions = cv_model.transform(test_data)\n\narea_under_roc = evaluator.evaluate(predictions)\nprint(f\"Area under ROC: {area_under_roc}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PANDAS","metadata":{}},{"cell_type":"code","source":"# Prepare the data (handle missing values, convert categorical features, etc.)\n\n# Split the data into features (X) and target variable (y)\nX = df.drop(\"target_column\", axis=1)  # Replace \"target_column\" with the actual target column name\ny = df[\"target_column\"]\n\n# Feature Engineering (if needed)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Build the RandomForest model\nrf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n\n# Perform cross-validation\ncv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='roc_auc')\n\n# Fit the model on the full training set\nrf.fit(X_train, y_train)\n\n# Make predictions on the test set\npredictions = rf.predict(X_test)\n\n# Evaluate the model\narea_under_roc = roc_auc_score(y_test, predictions)\nprint(f\"Area under ROC on test set: {area_under_roc}\")\n\n# Display cross-validation scores\nprint(\"Cross-Validation Scores:\", cv_scores)\nprint(f\"Mean Cross-Validation Score: {cv_scores.mean()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FINAL RESULTS","metadata":{}},{"cell_type":"code","source":"print(measures)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dsptlp/spark?scriptVersionId=167487492\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"1d31ec8f","metadata":{"papermill":{"duration":0.00676,"end_time":"2024-03-17T17:10:32.079014","exception":false,"start_time":"2024-03-17T17:10:32.072254","status":"completed"},"tags":[]},"source":["<img src=\"https://spark.apache.org/docs/3.1.3/api/python/_static/spark-logo-reverse.png\" alt=\"Sample Image\" width=\"200\"/>\n","\n","# SPARK \n","- Reasons to Use Spark\n","- This notebook will compare Spark VS scikit-learn\n","\n","# NOTE \n","- Spark is designed to work in a distributed computing environment and is most effective when dealing with large datasets and clusters of machines. \n","- In Kaggle's limited environment, we are not using a distributed computing environment but will be able to use all the computer resources which will be the only benefit. \n","\n","# SPARK ADVANTAGES\n","\n","1. **Speed:** Spark is known for its speed, as it can perform in-memory processing, reducing the need to write intermediate results to disk. This makes Spark well-suited for iterative algorithms and interactive data analysis.\n","\n","2. **Ease of Use:** Spark provides high-level APIs in languages such as Scala, Java, Python, and R, making it accessible to a wide range of users. It also offers built-in libraries for various tasks like SQL, machine learning (MLlib), graph processing (GraphX), and stream processing (Spark Streaming).\n","\n","3. **Scalability:** Spark is designed for distributed computing, allowing it to scale horizontally across a cluster of machines. This makes it suitable for handling large datasets and processing tasks that would be challenging for single-node systems.\n","\n","4. **Versatility:** Spark supports a variety of data processing scenarios, including batch processing, interactive queries, streaming analytics, and machine learning. This versatility makes it a preferred choice for organizations with diverse data analysis needs.\n","\n","5. **Fault Tolerance:** Spark provides fault tolerance through lineage information and resilient distributed datasets (RDDs). If a node fails, Spark can recompute the lost data using the lineage information, ensuring the reliability of data processing.\n","\n","6. **Integration with Big Data Ecosystem:** Spark seamlessly integrates with other big data tools and technologies, such as Hadoop Distributed File System (HDFS), Apache Hive, Apache HBase, and more. This allows users to leverage existing data storage and processing systems.\n","\n","7. **Community Support:** Spark has a large and active open-source community. This means continuous development, improvements, and a wealth of resources, including documentation, forums, and tutorials.\n","\n","8. **In-Memory Processing:** Spark's ability to store intermediate data in memory rather than writing to disk can significantly improve performance, especially for iterative algorithms and interactive data analysis, compared to traditional disk-based processing.\n"]},{"cell_type":"code","execution_count":1,"id":"dd6da0cc","metadata":{"execution":{"iopub.execute_input":"2024-03-17T17:10:32.094081Z","iopub.status.busy":"2024-03-17T17:10:32.093644Z","iopub.status.idle":"2024-03-17T17:11:26.291165Z","shell.execute_reply":"2024-03-17T17:11:26.289698Z"},"papermill":{"duration":54.213289,"end_time":"2024-03-17T17:11:26.29883","exception":false,"start_time":"2024-03-17T17:10:32.085541","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["pyspark not found. Installing...\n","pyspark installed successfully!\n"]}],"source":["# Install PySpark\n","try:\n","    import pyspark\n","except ImportError:\n","    print(\"pyspark not found. Installing...\")\n","    !pip install pyspark > pyspark.log.txt\n","    print(\"pyspark installed successfully!\")"]},{"cell_type":"code","execution_count":2,"id":"11cba7e5","metadata":{"execution":{"iopub.execute_input":"2024-03-17T17:11:26.31419Z","iopub.status.busy":"2024-03-17T17:11:26.313725Z","iopub.status.idle":"2024-03-17T17:11:29.466315Z","shell.execute_reply":"2024-03-17T17:11:29.465007Z"},"papermill":{"duration":3.163773,"end_time":"2024-03-17T17:11:29.469155","exception":false,"start_time":"2024-03-17T17:11:26.305382","status":"completed"},"tags":[]},"outputs":[],"source":["# Import necessary libraries\n","from pyspark.sql import SparkSession\n","import matplotlib.pyplot as plt\n","from sklearn import metrics\n","from sklearn.preprocessing import StandardScaler\n","from matplotlib.lines import Line2D\n","from matplotlib.colors import ListedColormap\n","\n","\n","from matplotlib import cm\n","import numpy as np \n","import pandas as pd\n","import seaborn as sns\n","import warnings\n","import timeit\n","\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml import Pipeline\n","from pyspark.ml.classification import RandomForestClassifier as SparkRFClassifier\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","from pyspark.ml.feature import PCA\n","from pyspark.ml.linalg import Vectors\n","from pyspark.sql.functions import col\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier as PandasRFClassifier\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import GridSearchCV\n","\n","import timeit\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA  as ScikitPCA\n","from sklearn.preprocessing import StandardScaler\n","\n","# Suppress all warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":3,"id":"4faeb63b","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-17T17:11:29.484518Z","iopub.status.busy":"2024-03-17T17:11:29.483931Z","iopub.status.idle":"2024-03-17T17:11:35.150866Z","shell.execute_reply":"2024-03-17T17:11:35.148962Z"},"papermill":{"duration":5.677956,"end_time":"2024-03-17T17:11:35.153936","exception":false,"start_time":"2024-03-17T17:11:29.47598","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/03/17 17:11:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]},{"name":"stdout","output_type":"stream","text":["Spark Version: 3.5.1\n"]}],"source":["# Create a Spark session\n","#spark = SparkSession.builder.appName(\"Spark\").getOrCreate()\n","\n","# Create a Spark session with local[*] master to use all available cores\n","spark = SparkSession.builder.master(\"local[*]\").appName(\"PCAExample\").getOrCreate()\n","\n","# Set log level to OFF \n","spark.sparkContext.setLogLevel(\"OFF\")\n","\n","# Print the Spark version\n","print(\"Spark Version:\", spark.version)"]},{"cell_type":"markdown","id":"705149d9","metadata":{"papermill":{"duration":0.007368,"end_time":"2024-03-17T17:11:35.168613","exception":false,"start_time":"2024-03-17T17:11:35.161245","status":"completed"},"tags":[]},"source":["# LOADING DATA"]},{"cell_type":"code","execution_count":4,"id":"9c1e78e1","metadata":{"execution":{"iopub.execute_input":"2024-03-17T17:11:35.186159Z","iopub.status.busy":"2024-03-17T17:11:35.184874Z","iopub.status.idle":"2024-03-17T17:11:35.191236Z","shell.execute_reply":"2024-03-17T17:11:35.190185Z"},"papermill":{"duration":0.017593,"end_time":"2024-03-17T17:11:35.193584","exception":false,"start_time":"2024-03-17T17:11:35.175991","status":"completed"},"tags":[]},"outputs":[],"source":["file_path  = \"/kaggle/input/tabular-dataset-ready-for-malicious-url-detection/test_dataset.csv\"\n","column_list = ['label','url_has_login','url_has_client','url_has_server','url_len']\n","\n","runs = 1\n","\n","measures = []\n","\n","row_count = 10000"]},{"cell_type":"markdown","id":"beffc493","metadata":{"papermill":{"duration":0.006894,"end_time":"2024-03-17T17:11:35.207612","exception":false,"start_time":"2024-03-17T17:11:35.200718","status":"completed"},"tags":[]},"source":["## SPARK"]},{"cell_type":"code","execution_count":5,"id":"d3fe715a","metadata":{"execution":{"iopub.execute_input":"2024-03-17T17:11:35.224599Z","iopub.status.busy":"2024-03-17T17:11:35.223891Z","iopub.status.idle":"2024-03-17T17:12:17.25139Z","shell.execute_reply":"2024-03-17T17:12:17.249885Z"},"papermill":{"duration":42.039794,"end_time":"2024-03-17T17:12:17.254887","exception":false,"start_time":"2024-03-17T17:11:35.215093","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Execution time using SPARK: 25.755976473000004 seconds\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["def load_csv_using_spark():\n","    df = spark.read.csv(file_path, header=True, inferSchema=True)\n","\n","    # Perform the summary: count number of records grouped by a column\n","    summary_df = df.groupBy(\"label\").count()\n","    \n","    # Perform an action (triggers execution, note that spark uses Lazy Execution)\n","    summary_df.collect() #show()\n","    \n","    return df\n","    \n","# Measure the execution time\n","execution_time = timeit.timeit(load_csv_using_spark, number=runs)\n","\n","# Print the result\n","print(f\"Execution time using SPARK: {execution_time} seconds\")\n","measures.append(('SPARK','load_csv',execution_time))\n","\n","spark_df = load_csv_using_spark()\n","spark_df = spark_df.select(column_list)"]},{"cell_type":"markdown","id":"8efb4f2c","metadata":{"papermill":{"duration":0.012059,"end_time":"2024-03-17T17:12:17.279202","exception":false,"start_time":"2024-03-17T17:12:17.267143","status":"completed"},"tags":[]},"source":["## PANDAS"]},{"cell_type":"code","execution_count":6,"id":"dfd484ad","metadata":{"execution":{"iopub.execute_input":"2024-03-17T17:12:17.304834Z","iopub.status.busy":"2024-03-17T17:12:17.304395Z","iopub.status.idle":"2024-03-17T17:12:48.859292Z","shell.execute_reply":"2024-03-17T17:12:48.857876Z"},"papermill":{"duration":31.571317,"end_time":"2024-03-17T17:12:48.862551","exception":false,"start_time":"2024-03-17T17:12:17.291234","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Execution time using PANDAS: 16.864486804999984 seconds\n"]}],"source":["def load_csv_using_pandas():\n","    data_df = pd.read_csv(file_path, delimiter=',') \n","    summary_df = data_df[['url_has_login','label']].groupby(['label']).count()\n","    return data_df\n","    \n","# Measure the execution time\n","execution_time = timeit.timeit(load_csv_using_pandas, number=runs)\n","\n","# Print the result\n","print(f\"Execution time using PANDAS: {execution_time} seconds\")\n","measures.append(('PANDAS','load_csv',execution_time))\n","\n","pandas_df = load_csv_using_pandas()\n","pandas_df = pandas_df[column_list]"]},{"cell_type":"markdown","id":"f03ed1ae","metadata":{"papermill":{"duration":0.00833,"end_time":"2024-03-17T17:12:48.880903","exception":false,"start_time":"2024-03-17T17:12:48.872573","status":"completed"},"tags":[]},"source":["# PCA"]},{"cell_type":"markdown","id":"3a71670b","metadata":{"papermill":{"duration":0.008231,"end_time":"2024-03-17T17:12:48.897652","exception":false,"start_time":"2024-03-17T17:12:48.889421","status":"completed"},"tags":[]},"source":["## SPARK"]},{"cell_type":"code","execution_count":7,"id":"dbab4b5c","metadata":{"execution":{"iopub.execute_input":"2024-03-17T17:12:48.916911Z","iopub.status.busy":"2024-03-17T17:12:48.915744Z","iopub.status.idle":"2024-03-17T17:13:04.578314Z","shell.execute_reply":"2024-03-17T17:13:04.577149Z"},"papermill":{"duration":15.675775,"end_time":"2024-03-17T17:13:04.581719","exception":false,"start_time":"2024-03-17T17:12:48.905944","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+---------------+------------------------------------------+\n","|Numfeatures    |pca_features                              |\n","+---------------+------------------------------------------+\n","|(4,[3],[15.0]) |[-14.999992762772708,0.014593764420371413]|\n","|(4,[3],[12.0]) |[-11.999994210218166,0.01167501153629713] |\n","|(4,[3],[12.0]) |[-11.999994210218166,0.01167501153629713] |\n","|(4,[3],[15.0]) |[-14.999992762772708,0.014593764420371413]|\n","|(4,[3],[29.0]) |[-28.999986008027236,0.028214611212718066]|\n","|(4,[3],[21.0]) |[-20.99998986788179,0.02043127018851998]  |\n","|(4,[3],[33.0]) |[-32.999984078099956,0.03210628172481711] |\n","|(4,[3],[25.0]) |[-24.99998793795451,0.024322940700619022] |\n","|(4,[3],[33.0]) |[-32.999984078099956,0.03210628172481711] |\n","|(4,[3],[21.0]) |[-20.99998986788179,0.02043127018851998]  |\n","|(4,[3],[35.0]) |[-34.99998311313632,0.03405211698086663]  |\n","|(4,[3],[13.0]) |[-12.999993727736346,0.012647929164321892]|\n","|(4,[3],[118.0])|[-117.9999430671453,0.11480428010692179]  |\n","|(4,[3],[20.0]) |[-19.99999035036361,0.019458352560495218] |\n","|(4,[3],[7.0])  |[-6.999996622627264,0.006810423396173326] |\n","|(4,[3],[23.0]) |[-22.99998890291815,0.0223771054445695]   |\n","|(4,[3],[32.0]) |[-31.999984560581776,0.03113336409679235] |\n","|(4,[3],[11.0]) |[-10.999994692699985,0.01070209390827237] |\n","|(4,[3],[23.0]) |[-22.99998890291815,0.0223771054445695]   |\n","|(4,[3],[26.0]) |[-25.99998745547269,0.025295858328643783] |\n","+---------------+------------------------------------------+\n","only showing top 20 rows\n","\n","Execution time using SPARK: 15.643446632000007 seconds\n"]}],"source":["def pca_csv_using_spark():\n","    \n","    numericColsAll  = ['url_has_login','url_has_client','url_has_server','url_len']\n","    label = 'label'\n","\n","    #VECTORIZE NUMERIC COLS\n","    assembler = VectorAssembler(inputCols=numericColsAll , outputCol=\"Numfeatures\")\n","    df = assembler.transform(spark_df)\n","\n","    # Apply PCA\n","    pca = PCA(k=2, inputCol=\"Numfeatures\", outputCol=\"pca_features\")\n","    model = pca.fit(df)\n","    result = model.transform(df)\n","\n","    # Show the result\n","    result.select(\"Numfeatures\", \"pca_features\").show(truncate=False)\n","\n","    # Extract data for plotting\n","    #features_and_labels = result.select(\"pca_features\").rdd.map(lambda row: (row.pca_features,))\n","\n","    # Collect transformed features and labels\n","    #features, = zip(*features_and_labels.collect())\n","\n","    # Extract labels from the original DataFrame\n","    #labels = result.select(label).rdd.map(lambda row: row[label]).collect()\n","\n","    # Create a color map with a unique color for each label\n","    #label_colors = ListedColormap(['red', 'green', 'blue', 'orange'])  # Add more colors if needed\n","\n","    # Plot the data points with different colors for each label\n","    #for i, label in enumerate(labels):\n","    #    plt.scatter(features[i][0], features[i][1], label=f\"Label {label}\", color=label_colors(label))\n","\n","    #plt.title(\"PCA Visualization with Labels\")\n","    #plt.xlabel(\"Principal Component 1\")\n","    #plt.ylabel(\"Principal Component 2\")\n","    #plt.legend()\n","    #plt.show()\n","    \n","# Measure the execution time\n","execution_time = timeit.timeit(pca_csv_using_spark, number=runs)\n","\n","# Print the result\n","print(f\"Execution time using SPARK: {execution_time} seconds\")\n","measures.append(('SPARK','PCA',execution_time))"]},{"cell_type":"markdown","id":"b3acb0d0","metadata":{"papermill":{"duration":0.013009,"end_time":"2024-03-17T17:13:04.608043","exception":false,"start_time":"2024-03-17T17:13:04.595034","status":"completed"},"tags":[]},"source":["## SCIKIT-LEARN"]},{"cell_type":"code","execution_count":8,"id":"077d9852","metadata":{"execution":{"iopub.execute_input":"2024-03-17T17:13:04.637709Z","iopub.status.busy":"2024-03-17T17:13:04.636683Z","iopub.status.idle":"2024-03-17T17:13:06.728045Z","shell.execute_reply":"2024-03-17T17:13:06.72661Z"},"papermill":{"duration":2.109216,"end_time":"2024-03-17T17:13:06.730696","exception":false,"start_time":"2024-03-17T17:13:04.62148","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Execution time using Scikit-learn: 2.078344448999985 seconds\n"]}],"source":["def pca_csv_using_scikit_learn():\n","\n","    numericColsAll  = ['url_has_login','url_has_client','url_has_server','url_len']\n","    label = 'label'\n","\n","    # Assuming your dataframe is 'pandas_df'\n","    # Extract the features and target variable\n","    X = pandas_df.drop(label, axis=1)  # Replace 'target_column' with the actual target column name\n","    y = pandas_df[label]\n","\n","    # Standardize the data (optional but recommended for PCA)\n","    from sklearn.preprocessing import StandardScaler\n","    X_std = StandardScaler().fit_transform(X)\n","\n","    # Apply PCA with 2 components\n","    pca = ScikitPCA(n_components=2)\n","    X_pca = pca.fit_transform(X_std)\n","\n","    # Create a DataFrame with the principal components and target variable\n","    df = pd.DataFrame(data=np.c_[X_pca, y], columns=['PC1', 'PC2', 'Target'])\n","    \n","# Measure the execution time\n","execution_time = timeit.timeit(pca_csv_using_scikit_learn, number=runs)\n","\n","# Print the result\n","print(f\"Execution time using Scikit-learn: {execution_time} seconds\")\n","measures.append(('Scikit','PCA',execution_time))"]},{"cell_type":"markdown","id":"9696e813","metadata":{"papermill":{"duration":0.00911,"end_time":"2024-03-17T17:13:06.74897","exception":false,"start_time":"2024-03-17T17:13:06.73986","status":"completed"},"tags":[]},"source":["# RANDOMFOREST"]},{"cell_type":"markdown","id":"1b74b3f2","metadata":{"papermill":{"duration":0.009259,"end_time":"2024-03-17T17:13:06.767531","exception":false,"start_time":"2024-03-17T17:13:06.758272","status":"completed"},"tags":[]},"source":["## SPARK"]},{"cell_type":"code","execution_count":9,"id":"73eaf992","metadata":{"execution":{"iopub.execute_input":"2024-03-17T17:13:06.788047Z","iopub.status.busy":"2024-03-17T17:13:06.787639Z","iopub.status.idle":"2024-03-17T17:33:37.484011Z","shell.execute_reply":"2024-03-17T17:33:37.48235Z"},"papermill":{"duration":1230.710962,"end_time":"2024-03-17T17:33:37.48804","exception":false,"start_time":"2024-03-17T17:13:06.777078","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 1699:==========================================>             (3 + 1) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["Area under ROC: 0.8389158513412673\n","Execution time using SPARK: 1230.568294791 seconds\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["def rf_using_spark():\n","    numericColsAll  = ['url_has_login','url_has_client','url_has_server','url_len']\n","    label = 'label'\n","\n","    #VECTORIZE NUMERIC COLS\n","    assembler = VectorAssembler(inputCols=numericColsAll , outputCol=\"Numfeatures\")\n","    df = assembler.transform(spark_df)\n","\n","    # Split the data into training and testing sets\n","    train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n","\n","    # Create the RandomForestClassifier\n","    rf = SparkRFClassifier(featuresCol=\"Numfeatures\", labelCol=\"label\")\n","\n","    # Create a pipeline\n","    pipeline = Pipeline(stages=[rf])\n","\n","    # Set up a parameter grid and cross-validator\n","    paramGrid = (ParamGridBuilder()\n","                 .addGrid(rf.numTrees, [10, 20, 30])\n","                 .addGrid(rf.maxDepth, [5, 10, 15])\n","                 .build())\n","\n","    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n","\n","    crossval = CrossValidator(estimator=pipeline,\n","                              estimatorParamMaps=paramGrid,\n","                              evaluator=evaluator,\n","                              numFolds=5)\n","\n","    # Fit the model\n","    cv_model = crossval.fit(train_data)\n","\n","    # Make predictions on the test set\n","    predictions = cv_model.transform(test_data)\n","\n","    # Evaluate the model\n","    area_under_roc = evaluator.evaluate(predictions)\n","    print(f\"Area under ROC: {area_under_roc}\")\n","    \n","# Measure the execution time\n","execution_time = timeit.timeit(rf_using_spark, number=runs)\n","\n","# Print the result\n","print(f\"Execution time using SPARK: {execution_time} seconds\")\n","measures.append(('SPARK','RF',execution_time))    "]},{"cell_type":"markdown","id":"128026a7","metadata":{"papermill":{"duration":0.107497,"end_time":"2024-03-17T17:33:37.701867","exception":false,"start_time":"2024-03-17T17:33:37.59437","status":"completed"},"tags":[]},"source":["## scikit-learn"]},{"cell_type":"code","execution_count":10,"id":"0e5b04f3","metadata":{"execution":{"iopub.execute_input":"2024-03-17T17:33:37.925597Z","iopub.status.busy":"2024-03-17T17:33:37.925111Z","iopub.status.idle":"2024-03-17T17:41:22.125346Z","shell.execute_reply":"2024-03-17T17:41:22.124068Z"},"papermill":{"duration":464.409116,"end_time":"2024-03-17T17:41:22.226021","exception":false,"start_time":"2024-03-17T17:33:37.816905","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Area under ROC: 0.7045601702155285\n","Execution time using SCIKIT: 464.17236283500006 seconds\n"]}],"source":["def rf_using_scikit():\n","\n","    # Split the data into features (X) and target variable (y)\n","    X = pandas_df.drop(\"label\", axis=1)  \n","    y = pandas_df[\"label\"]\n","\n","    # Split the data into training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # Define the parameter grid to search\n","    param_grid = {\n","        'n_estimators': [50],\n","        'max_depth': [5, 10, 15]\n","    }\n","\n","    # Build the RandomForest model\n","    rf = PandasRFClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n","\n","    # Create GridSearchCV\n","    grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc')\n","\n","    # Fit the model with the best parameters found by grid search\n","    grid_search.fit(X_train, y_train)\n","\n","    # Get the best parameters from the grid search\n","    best_params = grid_search.best_params_\n","\n","    # Get the best model from the grid search\n","    best_rf = grid_search.best_estimator_\n","\n","    # Perform cross-validation\n","    cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring='roc_auc')\n","\n","    # Fit the best model on the full training set\n","    best_rf.fit(X_train, y_train)\n","\n","    # Make predictions on the test set\n","    predictions = best_rf.predict(X_test)\n","\n","    # Evaluate the best model\n","    area_under_roc = roc_auc_score(y_test, predictions)\n","    print(f\"Area under ROC: {area_under_roc}\")\n","\n","        \n","# Measure the execution time\n","execution_time = timeit.timeit(rf_using_scikit, number=runs)\n","\n","# Print the result\n","print(f\"Execution time using SCIKIT: {execution_time} seconds\")\n","measures.append(('SCIKIT','RF',execution_time)) "]},{"cell_type":"markdown","id":"f45216fe","metadata":{"papermill":{"duration":0.09897,"end_time":"2024-03-17T17:41:22.425595","exception":false,"start_time":"2024-03-17T17:41:22.326625","status":"completed"},"tags":[]},"source":["# FINAL RESULTS"]},{"cell_type":"code","execution_count":11,"id":"087024e6","metadata":{"execution":{"iopub.execute_input":"2024-03-17T17:41:22.629669Z","iopub.status.busy":"2024-03-17T17:41:22.629188Z","iopub.status.idle":"2024-03-17T17:41:22.635766Z","shell.execute_reply":"2024-03-17T17:41:22.634529Z"},"papermill":{"duration":0.111775,"end_time":"2024-03-17T17:41:22.638291","exception":false,"start_time":"2024-03-17T17:41:22.526516","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[('SPARK', 'load_csv', 25.755976473000004), ('PANDAS', 'load_csv', 16.864486804999984), ('SPARK', 'PCA', 15.643446632000007), ('Scikit', 'PCA', 2.078344448999985), ('SPARK', 'RF', 1230.568294791), ('SCIKIT', 'RF', 464.17236283500006)]\n"]}],"source":["print(measures)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2529204,"sourceId":4295427,"sourceType":"datasetVersion"},{"datasetId":4441201,"sourceId":7624364,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":1856.332218,"end_time":"2024-03-17T17:41:25.364229","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-17T17:10:29.032011","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}
{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dsptlp/spark?scriptVersionId=163343051\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"6681d654","metadata":{"papermill":{"duration":0.002962,"end_time":"2024-02-18T19:29:17.14959","exception":false,"start_time":"2024-02-18T19:29:17.146628","status":"completed"},"tags":[]},"source":["# SPARK \n","- Reasons to Use Spark\n","- NOTE \n","\n","1. **Speed:** Spark is known for its speed, as it can perform in-memory processing, reducing the need to write intermediate results to disk. This makes Spark well-suited for iterative algorithms and interactive data analysis.\n","\n","2. **Ease of Use:** Spark provides high-level APIs in languages such as Scala, Java, Python, and R, making it accessible to a wide range of users. It also offers built-in libraries for various tasks like SQL, machine learning (MLlib), graph processing (GraphX), and stream processing (Spark Streaming).\n","\n","3. **Scalability:** Spark is designed for distributed computing, allowing it to scale horizontally across a cluster of machines. This makes it suitable for handling large datasets and processing tasks that would be challenging for single-node systems.\n","\n","4. **Versatility:** Spark supports a variety of data processing scenarios, including batch processing, interactive queries, streaming analytics, and machine learning. This versatility makes it a preferred choice for organizations with diverse data analysis needs.\n","\n","5. **Fault Tolerance:** Spark provides fault tolerance through lineage information and resilient distributed datasets (RDDs). If a node fails, Spark can recompute the lost data using the lineage information, ensuring the reliability of data processing.\n","\n","6. **Integration with Big Data Ecosystem:** Spark seamlessly integrates with other big data tools and technologies, such as Hadoop Distributed File System (HDFS), Apache Hive, Apache HBase, and more. This allows users to leverage existing data storage and processing systems.\n","\n","7. **Community Support:** Spark has a large and active open-source community. This means continuous development, improvements, and a wealth of resources, including documentation, forums, and tutorials.\n","\n","8. **In-Memory Processing:** Spark's ability to store intermediate data in memory rather than writing to disk can significantly improve performance, especially for iterative algorithms and interactive data analysis, compared to traditional disk-based processing.\n"]},{"cell_type":"code","execution_count":1,"id":"a9f484b3","metadata":{"execution":{"iopub.execute_input":"2024-02-18T19:29:17.156467Z","iopub.status.busy":"2024-02-18T19:29:17.156052Z","iopub.status.idle":"2024-02-18T19:30:04.861217Z","shell.execute_reply":"2024-02-18T19:30:04.860018Z"},"papermill":{"duration":47.71319,"end_time":"2024-02-18T19:30:04.86544","exception":false,"start_time":"2024-02-18T19:29:17.15225","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["pyspark not found. Installing...\n","pyspark installed successfully!\n"]}],"source":["# Install PySpark\n","try:\n","    import pyspark\n","except ImportError:\n","    print(\"pyspark not found. Installing...\")\n","    !pip install pyspark > pyspark.log.txt\n","    print(\"pyspark installed successfully!\")"]},{"cell_type":"code","execution_count":2,"id":"f2728183","metadata":{"execution":{"iopub.execute_input":"2024-02-18T19:30:04.872394Z","iopub.status.busy":"2024-02-18T19:30:04.872026Z","iopub.status.idle":"2024-02-18T19:30:04.955155Z","shell.execute_reply":"2024-02-18T19:30:04.954343Z"},"papermill":{"duration":0.089537,"end_time":"2024-02-18T19:30:04.9576","exception":false,"start_time":"2024-02-18T19:30:04.868063","status":"completed"},"tags":[]},"outputs":[],"source":["# Import necessary modules\n","from pyspark.sql import SparkSession"]},{"cell_type":"code","execution_count":3,"id":"3f642cc6","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-18T19:30:04.964899Z","iopub.status.busy":"2024-02-18T19:30:04.964079Z","iopub.status.idle":"2024-02-18T19:30:10.473475Z","shell.execute_reply":"2024-02-18T19:30:10.472382Z"},"papermill":{"duration":5.515436,"end_time":"2024-02-18T19:30:10.475932","exception":false,"start_time":"2024-02-18T19:30:04.960496","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/02/18 19:30:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]}],"source":["# Create a Spark session\n","spark = SparkSession.builder.appName(\"Spark\").getOrCreate()"]},{"cell_type":"code","execution_count":4,"id":"fb01ef38","metadata":{"execution":{"iopub.execute_input":"2024-02-18T19:30:10.483412Z","iopub.status.busy":"2024-02-18T19:30:10.483062Z","iopub.status.idle":"2024-02-18T19:30:10.489717Z","shell.execute_reply":"2024-02-18T19:30:10.489037Z"},"papermill":{"duration":0.012571,"end_time":"2024-02-18T19:30:10.491464","exception":false,"start_time":"2024-02-18T19:30:10.478893","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["'/kaggle/input/tabular-dataset-ready-for-malicious-url-detection'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["\"/kaggle/input/tabular-dataset-ready-for-malicious-url-detection\""]},{"cell_type":"code","execution_count":null,"id":"5b90d562","metadata":{"papermill":{"duration":0.002574,"end_time":"2024-02-18T19:30:10.496857","exception":false,"start_time":"2024-02-18T19:30:10.494283","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":5,"id":"32dde510","metadata":{"execution":{"iopub.execute_input":"2024-02-18T19:30:10.503949Z","iopub.status.busy":"2024-02-18T19:30:10.503425Z","iopub.status.idle":"2024-02-18T19:30:19.064504Z","shell.execute_reply":"2024-02-18T19:30:19.063625Z"},"papermill":{"duration":8.568144,"end_time":"2024-02-18T19:30:19.067729","exception":false,"start_time":"2024-02-18T19:30:10.499585","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-----+---+\n","| Name|Age|\n","+-----+---+\n","| John| 25|\n","|Alice| 30|\n","|  Bob| 22|\n","+-----+---+\n","\n"]}],"source":["# Create a simple DataFrame and display it\n","data = [(\"John\", 25), (\"Alice\", 30), (\"Bob\", 22)]\n","columns = [\"Name\", \"Age\"]\n","df = spark.createDataFrame(data, columns)\n","\n","df.show()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4441201,"sourceId":7624364,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":67.170059,"end_time":"2024-02-18T19:30:21.690292","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-18T19:29:14.520233","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}
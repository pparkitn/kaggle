{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4295427,"sourceType":"datasetVersion","datasetId":2529204},{"sourceId":7624364,"sourceType":"datasetVersion","datasetId":4441201}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dsptlp/spark?scriptVersionId=163781797\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<img src=\"https://spark.apache.org/docs/3.1.3/api/python/_static/spark-logo-reverse.png\" alt=\"Sample Image\" width=\"200\"/>\n\n# SPARK \n- Reasons to Use Spark\n- This notebook will compare Spark VS scikit-learn\n\n# NOTE \n- Spark is designed to work in a distributed computing environment and is most effective when dealing with large datasets and clusters of machines. \n- In Kaggle's limited environment, we are not using a distributed computing environment but will be able to use all the computer resources which will be the only benefit. \n\n# SPARK ADVANTAGES\n\n1. **Speed:** Spark is known for its speed, as it can perform in-memory processing, reducing the need to write intermediate results to disk. This makes Spark well-suited for iterative algorithms and interactive data analysis.\n\n2. **Ease of Use:** Spark provides high-level APIs in languages such as Scala, Java, Python, and R, making it accessible to a wide range of users. It also offers built-in libraries for various tasks like SQL, machine learning (MLlib), graph processing (GraphX), and stream processing (Spark Streaming).\n\n3. **Scalability:** Spark is designed for distributed computing, allowing it to scale horizontally across a cluster of machines. This makes it suitable for handling large datasets and processing tasks that would be challenging for single-node systems.\n\n4. **Versatility:** Spark supports a variety of data processing scenarios, including batch processing, interactive queries, streaming analytics, and machine learning. This versatility makes it a preferred choice for organizations with diverse data analysis needs.\n\n5. **Fault Tolerance:** Spark provides fault tolerance through lineage information and resilient distributed datasets (RDDs). If a node fails, Spark can recompute the lost data using the lineage information, ensuring the reliability of data processing.\n\n6. **Integration with Big Data Ecosystem:** Spark seamlessly integrates with other big data tools and technologies, such as Hadoop Distributed File System (HDFS), Apache Hive, Apache HBase, and more. This allows users to leverage existing data storage and processing systems.\n\n7. **Community Support:** Spark has a large and active open-source community. This means continuous development, improvements, and a wealth of resources, including documentation, forums, and tutorials.\n\n8. **In-Memory Processing:** Spark's ability to store intermediate data in memory rather than writing to disk can significantly improve performance, especially for iterative algorithms and interactive data analysis, compared to traditional disk-based processing.\n","metadata":{}},{"cell_type":"code","source":"# Install PySpark\ntry:\n    import pyspark\nexcept ImportError:\n    print(\"pyspark not found. Installing...\")\n    !pip install pyspark > pyspark.log.txt\n    print(\"pyspark installed successfully!\")","metadata":{"execution":{"iopub.status.busy":"2024-02-22T02:14:43.481112Z","iopub.execute_input":"2024-02-22T02:14:43.481501Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"pyspark not found. Installing...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import necessary libraries\nfrom pyspark.sql import SparkSession\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom matplotlib.lines import Line2D\nfrom matplotlib import cm\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport warnings\nimport timeit\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier as SparkRFClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier as PandasRFClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Suppress all warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a Spark session\nspark = SparkSession.builder.appName(\"Spark\").getOrCreate()\n\n# Set log level to OFF \nspark.sparkContext.setLogLevel(\"OFF\")\n\n# Print the Spark version\nprint(\"Spark Version:\", spark.version)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path  = \"/kaggle/input/tabular-dataset-ready-for-malicious-url-detection/train_dataset.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LOADING DATA","metadata":{}},{"cell_type":"code","source":"measures = []\ncolumn_list = ['label','url_has_login','url_has_client','url_has_server','url_len']\nruns = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SPARK","metadata":{}},{"cell_type":"code","source":"def load_csv_using_spark():\n    df = spark.read.csv(file_path, header=True, inferSchema=True)\n\n    # Perform the summary: count number of records grouped by a column\n    summary_df = df.groupBy(\"label\").count()\n    \n    # Perform an action (triggers execution, note that spark uses Lazy Execution)\n    summary_df.collect() #show()\n    \n    return df\n    \n# Measure the execution time\nexecution_time = timeit.timeit(load_csv_using_spark, number=runs)\n\n# Print the result\nprint(f\"Execution time using SPARK: {execution_time} seconds\")\nmeasures.append(('SPARK','load_csv',execution_time))\n\nspark_df = load_csv_using_spark()\nspark_df = spark_df.select(column_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PANDAS","metadata":{}},{"cell_type":"code","source":"def load_csv_using_pandas():\n    data_df = pd.read_csv(file_path, delimiter=',') \n    summary_df = data_df[['url_has_login','label']].groupby(['label']).count()\n    return data_df\n    \n# Measure the execution time\nexecution_time = timeit.timeit(load_csv_using_pandas, number=runs)\n\n# Print the result\nprint(f\"Execution time using PANDAS: {execution_time} seconds\")\nmeasures.append(('PANDAS','load_csv',execution_time))\n\npandas_df = load_csv_using_pandas()\npandas_df = pandas_df[column_list]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA","metadata":{}},{"cell_type":"markdown","source":"## SPARK","metadata":{}},{"cell_type":"code","source":"# Apply PCA\npca = PCA(k=3, inputCol=\"features\", outputCol=\"pca_features\")\nmodel = pca.fit(spark_df)\nresult = model.transform(spark_df)\n\n# Show the result\nresult.select(\"features\", \"pca_features\").show(truncate=False)\n\n# Extract data for plotting\nfeatures_and_labels = result.select(\"pca_features\").rdd.map(lambda row: (row.pca_features,))\n\n# Collect transformed features and labels\nfeatures, = zip(*features_and_labels.collect())\n\n# Extract labels from the original DataFrame\nlabels = result.select(\"features\").rdd.map(lambda row: row.features).collect()\n\n# Plot the data points with different colors for each label\nfor i, label in enumerate(labels):\n    plt.scatter(features[i][0], features[i][1], label=f\"Label {i + 1}\")\n\nplt.title(\"PCA Visualization with Labels\")\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SCIKIT-LEARN","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RANDOMFOREST","metadata":{}},{"cell_type":"markdown","source":"## SPARK","metadata":{}},{"cell_type":"code","source":"def rf_using_spark():\n    numericColsAll  = ['url_has_login','url_has_client','url_has_server','url_len']\n    label = 'label'\n\n    #VECTORIZE NUMERIC COLS\n    assembler = VectorAssembler(inputCols=numericColsAll , outputCol=\"Numfeatures\")\n    df = assembler.transform(spark_df)\n\n    # Split the data into training and testing sets\n    train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n\n    # Create the RandomForestClassifier\n    rf = SparkRFClassifier(featuresCol=\"Numfeatures\", labelCol=\"label\")\n\n    # Create a pipeline\n    pipeline = Pipeline(stages=[rf])\n\n    # Set up a parameter grid and cross-validator\n    paramGrid = (ParamGridBuilder()\n                 .addGrid(rf.numTrees, [10, 20, 30])\n                 .addGrid(rf.maxDepth, [5, 10, 15])\n                 .build())\n\n    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n\n    crossval = CrossValidator(estimator=pipeline,\n                              estimatorParamMaps=paramGrid,\n                              evaluator=evaluator,\n                              numFolds=5)\n\n    # Fit the model\n    cv_model = crossval.fit(train_data)\n\n    # Make predictions on the test set\n    predictions = cv_model.transform(test_data)\n\n    # Evaluate the model\n    area_under_roc = evaluator.evaluate(predictions)\n    print(f\"Area under ROC: {area_under_roc}\")\n    \n# Measure the execution time\nexecution_time = timeit.timeit(rf_using_spark, number=runs)\n\n# Print the result\nprint(f\"Execution time using SPARK: {execution_time} seconds\")\nmeasures.append(('SPARK','RF',execution_time))    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## scikit-learn","metadata":{}},{"cell_type":"code","source":"def rf_using_scikit():\n\n    # Split the data into features (X) and target variable (y)\n    X = pandas_df.drop(\"label\", axis=1)  \n    y = pandas_df[\"label\"]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Define the parameter grid to search\n    param_grid = {\n        'n_estimators': [50],\n        'max_depth': [5, 10, 15]\n    }\n\n    # Build the RandomForest model\n    rf = PandasRFClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n\n    # Create GridSearchCV\n    grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc')\n\n    # Fit the model with the best parameters found by grid search\n    grid_search.fit(X_train, y_train)\n\n    # Get the best parameters from the grid search\n    best_params = grid_search.best_params_\n\n    # Get the best model from the grid search\n    best_rf = grid_search.best_estimator_\n\n    # Perform cross-validation\n    cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring='roc_auc')\n\n    # Fit the best model on the full training set\n    best_rf.fit(X_train, y_train)\n\n    # Make predictions on the test set\n    predictions = best_rf.predict(X_test)\n\n    # Evaluate the best model\n    area_under_roc = roc_auc_score(y_test, predictions)\n    print(f\"Area under ROC: {area_under_roc}\")\n\n        \n# Measure the execution time\nexecution_time = timeit.timeit(rf_using_scikit, number=runs)\n\n# Print the result\nprint(f\"Execution time using SCIKIT: {execution_time} seconds\")\nmeasures.append(('SCIKIT','RF',execution_time)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FINAL RESULTS","metadata":{}},{"cell_type":"code","source":"print(measures)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
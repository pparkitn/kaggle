{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2047221,"sourceType":"datasetVersion","datasetId":1226038}],"dockerImageVersionId":30154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dsptlp/classificationtosql?scriptVersionId=163260232\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Classification\n- Build Classification Model\n- Extract features and build SQL Code Implementation\n\n## Key Features:\n### Data Loading and Preprocessing:\n\n- The script loads a heart-related dataset from a CSV file, handles missing values, and encodes categorical variables.\n- Robust scaling is applied to continuous features.\n\n### Model Training and Evaluation:\n\n- Several classification models (e.g., AdaBoost, Logistic Regression, Random Forest, Gradient Boosting) are trained and evaluated.\n- Model evaluation includes accuracy, confusion matrix, and ROC AUC. Cross-validation is implemented for more robust performance assessment.\n\n### Hyperparameter Tuning:\n\n- Hyperparameter tuning is performed for RandomForestClassifier and LogisticRegression using GridSearchCV.\n\n### Feature Importance (Tree-based Models):\n\n- For tree-based models, feature importance is extracted and displayed.\n\n### Model Saving:\n\n- Trained models are saved using joblib for potential deployment.\n","metadata":{}},{"cell_type":"code","source":"# Importing necessary packages\nimport os\nimport numpy as np\nimport pandas as pd\nimport logging\nimport warnings\nimport joblib\n\nfrom sklearn.metrics import classification_report, roc_curve, accuracy_score, confusion_matrix, roc_auc_score\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import RobustScaler\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\n\n# Setting up options and ignoring warnings\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\n# Set the logging level to INFO\nlogging.basicConfig(level=logging.INFO)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the dataset\n\ntry:\n    # Attempt to read the dataset\n    df = pd.read_csv(\"../input/heart-attack-analysis-prediction-dataset/heart.csv\")\nexcept FileNotFoundError:\n    print(\"Error: Dataset file not found. Please provide the correct file path.\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a copy of df\ndf1 = df.fillna(0)\n\n# define the columns to be encoded and scaled\ncat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\n# encoding the categorical columns\ndf1 = pd.get_dummies(df1, columns = cat_cols, drop_first = True)\n\n# defining the features and target\nX = df1.drop(['output'],axis=1)\ny = df1[['output']]\n\n# instantiating the scaler\nscaler = RobustScaler()\n\n# scaling the continuous featuree\nX[con_cols] = scaler.fit_transform(X[con_cols])\n\n# Get centering and scaling values for each feature\ncentering = scaler.center_\niqrs  = scaler.scale_\n\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a dictionary of classification models\nclassifiers = {\n    'AdaBoostClassifier': AdaBoostClassifier(),\n    'LogisticRegression': LogisticRegression(random_state=9),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'RandomForestClassifier': RandomForestClassifier(n_estimators=100, random_state=0),\n    'GradientBoostingClassifier': GradientBoostingClassifier(n_estimators=300, max_depth=1, subsample=0.8, max_features=0.2),\n    'XGBClassifier': xgb.XGBClassifier(),\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Improved model evaluation and cross-validation\nmodel_results = []\n\n# Looping through models and evaluating their performance\nfor name, model in classifiers.items():\n    \n    # Cross-validation\n    cv_scores = cross_val_score(model, X, y.values.ravel(), cv=5, scoring='accuracy')\n    \n    # Model training\n    model.fit(X_train, y_train)\n    predict = model.predict(X_test)\n    acc = accuracy_score(y_test, predict)\n    conf = confusion_matrix(y_test, predict)\n\n    y_pred_proba = model.predict_proba(X_test)[:, 1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n    roc_auc = roc_auc_score(y_test, y_pred_proba)\n\n    model_results.append({'Model': name, 'Accuracy': acc, 'Confusion Matrix': conf, 'ROC AUC': roc_auc, 'CV Scores': cv_scores})\n    \n    print(\"=============================================================================\")\n    print(name, \"CV Scores:\", cv_scores)\n    print(name, \"Accuracy:\", acc)\n    print(name, \"Confusion Matrix:\", conf)\n    print(name, \"ROC AUC:\", roc_auc)\n    print(classification_report(y_test, predict))\n\n    # Plotting the ROC curve\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.plot(fpr, tpr, label='Knn')\n    plt.xlabel('fpr')\n    plt.ylabel('tpr')\n    plt.title('ROC curve')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature importance for tree-based models\nfor name, model in classifiers.items():\n    if isinstance(model, (RandomForestClassifier, GradientBoostingClassifier)):\n        model.fit(X_train, y_train)\n        feature_importances = model.feature_importances_\n        feature_names = X_train.columns\n        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n        print(f\"{name} - Feature Importance:\")\n        print(importance_df.sort_values(by='Importance', ascending=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save trained models for deployment\nfor name, model in classifiers.items():\n    joblib.dump(model, f\"{name}_model.joblib\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning RandomForest","metadata":{}},{"cell_type":"code","source":"# instantiating the object\nmodel = RandomForestClassifier()\n\n# setting a grid - not so extensive\nparameters = {'n_estimators': [50, 60],\n    'max_depth': [2,3,10,15,20,25, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]}\n\n# instantiating the GridSearchCV object\n# For binary classification:\n# 'accuracy': Calculates the accuracy of the classifier.\n# 'precision': Measures the ability of the classifier not to label as positive a sample that is negative.\n# 'recall': Measures the ability of the classifier to capture all the positive samples.\n# 'f1': Combines precision and recall into a single metric.\n# 'roc_auc': Computes the area under the Receiver Operating Characteristic (ROC) curve.\n\nsearcher = GridSearchCV(estimator = model, param_grid = parameters,cv=5, scoring='accuracy')\n\n# fitting the object\nsearcher.fit(X_train, y_train)\n\n# the scores\nprint(\"The best params are :\", searcher.best_params_)\nprint(\"The best score is   :\", searcher.best_score_)\n\n# predicting the values\ny_pred = searcher.predict(X_test)\n\n# printing the test accuracy\nprint(\"The test accuracy score of model after hyper-parameter tuning is \", accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning LOGREG","metadata":{}},{"cell_type":"code","source":"# instantiating the object\nmodel = LogisticRegression()\n\n# Define a parameter grid\nparameters = {\n    'C': np.logspace(-3, 3,4, 5,7),  # Regularization parameter\n    'penalty': ['None','l1', 'l2','elasticnet'],       # Regularization type\n    'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],        # Solver algorithm\n    'max_iter' :[50,100,200,300],\n    'n_jobs' :[-1]\n}\n\n# instantiating the GridSearchCV object\n# For binary classification:\n# 'accuracy': Calculates the accuracy of the classifier.\n# 'precision': Measures the ability of the classifier not to label as positive a sample that is negative.\n# 'recall': Measures the ability of the classifier to capture all the positive samples.\n# 'f1': Combines precision and recall into a single metric.\n# 'roc_auc': Computes the area under the Receiver Operating Characteristic (ROC) curve.\n\nsearcher = GridSearchCV(estimator = model, param_grid = parameters,cv=5, scoring='accuracy')\n\n# fitting the object\nsearcher.fit(X_train, y_train)\n\n# Get the best hyperparameters from the GridSearchCV\nbest_params = searcher.best_params_\n\n# the scores\nprint(\"The best params are :\", best_params)\nprint(\"The best score is   :\", searcher.best_score_)\n\n# predicting the values\ny_pred = searcher.predict(X_test)\n\n# printing the test accuracy\nprint(\"The test accuracy score of model after hyper-parameter tuning is \", accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TURNING LOGREG MODEL INTO SQL IMPLEMENTATION","metadata":{}},{"cell_type":"code","source":"print(best_params)\nmodel=LogisticRegression(**best_params)\nmodel.fit(X_train,y_train)\npredict = model.predict(X_test)\nacc = accuracy_score(y_test, predict)\nconf = confusion_matrix(y_test, predict)\n\ny_pred_proba = model.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nroc_auc = roc_auc_score(y_test,y_pred_proba)\n\nprint(name,acc)\nprint(conf)    \nprint(roc_auc)\nprint(classification_report(y_test,predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the model coefficients\ncoefficients = model.coef_[0]\n\n# Get the feature names\nfeature_names = X_train.columns\n\n# Create a DataFrame to display the coefficients along with the feature names\ncoefficients_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n\nintercept = model.intercept_[0]\nprint('Intercept =',intercept)\n\n# Print the DataFrame\nprint(coefficients_df)\ncoefficients_list = coefficients_df.values.tolist()\n\n# Get centering and scaling values for each feature\nprint(\"Centering\",centering)\nprint(\"iqrs\",iqrs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_Scored = df1\ndf1_Scored['prob_Manual'] =             1/(1+np.exp(-(intercept +  \\\n                                        coefficients_list[0][1] * (df1_Scored[coefficients_list[0][0]] - centering[0] ) / iqrs[0] + \\\n                                        coefficients_list[1][1] * (df1_Scored[coefficients_list[1][0]] - centering[1] ) / iqrs[1] + \\\n                                        coefficients_list[2][1] * (df1_Scored[coefficients_list[2][0]] - centering[2] ) / iqrs[2] + \\\n                                        coefficients_list[3][1] * (df1_Scored[coefficients_list[3][0]] - centering[3] ) / iqrs[3] + \\\n                                        coefficients_list[4][1] * (df1_Scored[coefficients_list[4][0]] - centering[4] ) / iqrs[4] + \\\n                                        coefficients_list[5][1] * (df1_Scored[coefficients_list[5][0]]) +  \\\n                                        coefficients_list[6][1] * (df1_Scored[coefficients_list[6][0]]) +  \\\n                                        coefficients_list[7][1] * (df1_Scored[coefficients_list[7][0]]) +  \\\n                                        coefficients_list[8][1] * (df1_Scored[coefficients_list[8][0]]) +  \\\n                                        coefficients_list[9][1] * (df1_Scored[coefficients_list[9][0]]) +  \\\n                                        coefficients_list[10][1] * (df1_Scored[coefficients_list[10][0]]) +  \\\n                                        coefficients_list[11][1] * (df1_Scored[coefficients_list[11][0]]) +  \\\n                                        coefficients_list[12][1] * (df1_Scored[coefficients_list[12][0]]) +  \\\n                                        coefficients_list[13][1] * (df1_Scored[coefficients_list[13][0]]) +  \\\n                                        coefficients_list[14][1] * (df1_Scored[coefficients_list[14][0]]) +  \\\n                                        coefficients_list[15][1] * (df1_Scored[coefficients_list[15][0]]) +  \\\n                                        coefficients_list[16][1] * (df1_Scored[coefficients_list[16][0]]) +  \\\n                                        coefficients_list[17][1] * (df1_Scored[coefficients_list[17][0]]) +  \\\n                                        coefficients_list[18][1] * (df1_Scored[coefficients_list[18][0]]) +  \\\n                                        coefficients_list[19][1] * (df1_Scored[coefficients_list[19][0]]) +  \\\n                                        coefficients_list[19][1] * (df1_Scored[coefficients_list[20][0]]) +  \\\n                                        coefficients_list[19][1] * (df1_Scored[coefficients_list[21][0]]) +  \\\n                                                      0 )))\n                                                                                              \nroc_auc = roc_auc_score(df1_Scored['output'],df1_Scored['prob_Manual'])\nprint(roc_auc)\ndf1_Scored[df1_Scored['output'] == 0].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sql_table_name = ''\ncolumn_name = ''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nSELECT\n  1 / (1 + EXP(-(\n    -- Intercept\n    <intercept> +\n    -- Coefficients for each feature\n    <coef_1> * (feature_1 - <median_1>) / <iqr_1> +\n    <coef_2> * (feature_2 - <median_2>) / <iqr_2> +\n    -- ... Repeat for all features ...\n    <coef_n> * (feature_n - <median_n>) / <iqr_n>\n  ))) AS predicted_probability\nFROM your_table_with_unscaled_features;\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
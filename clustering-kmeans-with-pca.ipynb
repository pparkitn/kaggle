{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":974,"sourceType":"datasetVersion","datasetId":478},{"sourceId":7384155,"sourceType":"datasetVersion","datasetId":4291772}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dsptlp/clustering-kmeans-with-pca?scriptVersionId=162513879\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Clustering Using Kmeans\n## Example 1 - Apple Quality\n - Description\n - Data Source \n## Example 2 - Mushrooms\n - Description\n - Data Source ","metadata":{}},{"cell_type":"code","source":"!pip install polar >>install.log","metadata":{"execution":{"iopub.status.busy":"2024-02-11T14:57:45.777289Z","iopub.execute_input":"2024-02-11T14:57:45.777673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.decomposition import PCA\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import StandardScaler\nfrom matplotlib.colors import LogNorm\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nimport warnings\nimport polar as pl\n\n# Suppress all warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXAMPLE 1 - Apple Quality","metadata":{}},{"cell_type":"code","source":"data_df = pd.read_csv('/kaggle/input/apple-quality/apple_quality.csv', delimiter=',') \nprint(len(data_df))\ndata_df = data_df.drop('A_id', axis=1)\ndata_df = data_df[data_df['Acidity'] != 'Created_by_Nidula_Elgiriyewithana']\ndata_df.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data_corr_df = pl.analyze_correlation(data_df,'Quality')\n#pl.get_heatmap(data_corr_df,'correlation_heat_map.png',1.1,14,'0.1f',0,100,5,5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_theme()\nplt.figure()\nsns.pairplot(data_df.replace([np.inf, -np.inf], np.nan), hue=\"Quality\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scalar = StandardScaler()\ndf_std = scalar.fit_transform(data_df.drop('Quality', axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert these lists to numpy arrays.\nX = np.array(df_std)\nY = np.array(data_df['Quality'])\n\n# Split into train and test data.\ntrain_data, train_labels = X[:], Y[:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vals = []\n\n# Running for all possible values of k\nk = range(1,(len(train_data[1])+1),1)\nfor comp_int in k:\n\n    # Create PCA Model\n    model = PCA(n_components = comp_int)\n    model.fit(train_data)\n    explained = np.sum(model.explained_variance_ratio_)\n    print(\"Fraction of the total variance in the training data that is explained by %0.0f\" %comp_int ,\"components is = %0.2f %%\" %(explained*100))\n    vals.append(explained)\n\n#Graph Results    \nfig=plt.figure(figsize=(12,5))\nplt.plot(k,vals,color='red')\nplt.xlabel(\"Principal Components\")\nplt.ylabel(\"Fraction of the total variance\")\nplt.title(\"Fraction of the total variance in the training data VS Principal Components\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Build the PCA Model\nmodel_pca = PCA(n_components = (len(train_data[1])))\ntrain_data_pca = model_pca.fit_transform(train_data)\n\n# Quick examination of elbow method to find numbers of clusters to make.\nprint('Elbow Method to determine the number of clusters to be formed:')\nElbow_M = KElbowVisualizer(KMeans(n_init=10), k=7)\nElbow_M.fit(train_data_pca)\nElbow_M.show(outpath=\"elbow_plot.png\")\n\noptimal_k = Elbow_M.elbow_value_\nprint(\"Optimal number of clusters:\", optimal_k)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the KMeans Model\nmodel_kmeans = KMeans(n_clusters=optimal_k,init = 'k-means++',n_init='auto')\nkmeans_clusters = model_kmeans.fit(train_data)\nkmeans_clusters\n\ndata_df['kmean_clusters'] = kmeans_clusters.labels_\ndata_df.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Build the PCA Model 2 DIMS\nmodel_pca = PCA(n_components = 2)\ntrain_data2DIM_pca = model_pca.fit_transform(train_data)\ntrain_data2DIM_pca = pd.DataFrame(train_data2DIM_pca)\ntrain_data2DIM_pca.columns = ['pca_2x','pca_2y']\ntrain_data2DIM_pca.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Build the PCA Model 3 DIMS\nmodel_pca = PCA(n_components = 3)\ntrain_data3DIM_pca = model_pca.fit_transform(train_data)\ntrain_data3DIM_pca = pd.DataFrame(train_data3DIM_pca)\ntrain_data3DIM_pca.columns = ['pca_3x','pca_3y','pca_3z']\ntrain_data3DIM_pca.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ff = pd.merge(data_df, train_data2DIM_pca, left_index=True, right_index=True)\nff = pd.merge(ff, train_data3DIM_pca, left_index=True, right_index=True)\nff","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.lines import Line2D\nfrom matplotlib import cm\n\n#Plotting the clusters\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111, projection='3d')\nax.scatter(ff[\"pca_3x\"], ff[\"pca_3y\"], ff[\"pca_3z\"], s=20, c=ff[\"kmean_clusters\"], marker='o',cmap='viridis' )\nax.set_title(\"Clusters represented Via PCA in 3D\")\n\n# Creating a legend based on unique cluster values\nlegend_labels = ff[\"kmean_clusters\"].unique()\nlegend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=cm.viridis(label), markersize=10) for label in legend_labels]\nax.legend(legend_elements, legend_labels, title='Cluster')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have a dataframe 'ff' and it has a column 'Quality'\n# Convert 'Quality' to categorical for color mapping\nff['Quality'] = ff['Quality'].astype('category')\ncategories = ff['Quality'].cat.categories\n\n# Plotting the clusters\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# Assigning unique colors to each category\ncolors = plt.cm.viridis(range(len(categories)))\n\n# Scatter plot with categorical colors\nscatter = ax.scatter(\n    ff[\"pca_3x\"], ff[\"pca_3y\"], ff[\"pca_3z\"],\n    s=20, c=ff['Quality'].cat.codes, cmap='viridis', marker='o'\n)\n\nax.set_title(\"Clusters represented Via PCA in 3D\")\n\n# Adding colorbar for better visualization of category values\ncbar = plt.colorbar(scatter)\ncbar.set_ticks(range(len(categories)))\ncbar.set_ticklabels(categories)\ncbar.set_label('Quality')\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ff.loc[data_df[\"Quality\"] == \"good\", \"QualityBin\"] = 1\nff.loc[data_df[\"Quality\"] == \"bad\", \"QualityBin\"] = 0\nff","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = [ \"Size\",\"Weight\",\"Sweetness\",\"Crunchiness\",\"Juiciness\",\"Ripeness\"]\n\n# Number of clusters or unique values in the \"kmean_clusters\" variable\nnum_clusters = len(ff[\"kmean_clusters\"].unique())\n\n# Define a custom color palette with distinct colors for each cluster\ncustom_palette = sns.color_palette(\"husl\", n_colors=num_clusters)\n\n# Assuming ff is your DataFrame\nfor i in columns:\n    plt.figure()\n    sns.jointplot(x=ff[i], y=ff[\"QualityBin\"], hue=ff[\"kmean_clusters\"], kind=\"kde\", palette=custom_palette)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = [ \"Size\",\"Weight\",\"Sweetness\",\"Crunchiness\",\"Juiciness\",\"Ripeness\"]\n\nfor i in columns:\n    plt.figure()\n    sns.jointplot(x=ff[i], y=ff[\"QualityBin\"], hue =ff[\"kmean_clusters\"])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXAMPLE 2 - Mushrooms","metadata":{}},{"cell_type":"code","source":"data_df = pd.read_csv('/kaggle/input/mushroom-classification/mushrooms.csv', delimiter=',') \nprint(len(data_df))\ndata_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df.loc[data_df[\"class\"] == \"p\", \"class\"] = 1\ndata_df.loc[data_df[\"class\"] == \"e\", \"class\"] = 0\ndata_df['class'] = data_df['class'].astype(int)\ndata_df.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_corr_df = pl.analyze_association(data_df,'class',verbose=0)\npl.get_heatmap(data_corr_df,'association_heat_map.png',1.1,12,'0.1f',0,100,10,10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform one-hot encoding using get_dummies\ndf_encoded = pd.get_dummies(data_df)\ndf_encoded.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_encoded.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert these lists to numpy arrays.\nX = np.array(df_encoded.drop('class', axis=1))\nY = np.array(df_encoded['class'])\n\n# Split into train and test data.\ntrain_data, train_labels = X[:7000], Y[:7000]\ntest_data, test_labels = X[7000:], Y[7000:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vals = []\n\n# Running for all possible values of k\nk = range(1,(len(train_data[1])+1),1)\nfor comp_int in k:\n\n    # Create PCA Model\n    model = PCA(n_components = comp_int)\n    model.fit(train_data)\n    explained = np.sum(model.explained_variance_ratio_)\n    print(\"Fraction of the total variance in the training data that is explained by %0.0f\" %comp_int ,\"components is = %0.2f %%\" %(explained*100))\n    vals.append(explained)\n\n#Graph Results    \nfig=plt.figure(figsize=(12,5))\nplt.plot(k,vals,color='red')\nplt.xlabel(\"Principal Components\")\nplt.ylabel(\"Fraction of the total variance\")\nplt.title(\"Fraction of the total variance in the training data VS Principal Components\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Build the PCA Model\nmodel_pca = PCA(n_components = 8)\ntrain_data_2dims = model_pca.fit_transform(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_pca = model_pca.transform(train_data)\nscores_pcs_df = pd.DataFrame(scores_pca)\nscores_pcs_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick examination of elbow method to find numbers of clusters to make.\nprint('Elbow Method to determine the number of clusters to be formed:')\nElbow_M = KElbowVisualizer(KMeans(n_init=10), k=10)\nElbow_M.fit(train_data_2dims)\nElbow_M.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build PCA Model with 2 PCA Components\nmodel = PCA(n_components = 2)\ntrain_data_2dims = model.fit_transform(train_data)\n\n# Graph Results\nfig=plt.figure(figsize=(10,10))\nplt.scatter(train_data_2dims[:,0][train_labels==1],train_data_2dims[:,1][train_labels==1],marker=\"o\",color='red', label=\"poisonous\",edgecolor='black',s=35,alpha=0.7)\nplt.scatter(train_data_2dims[:,0][train_labels==0],train_data_2dims[:,1][train_labels==0],marker=\"o\",color='green', label=\"non-poisonous\",edgecolor='black',s=35,alpha=0.7)    \nplt.xlabel(\"First PCA Dimension\")\nplt.ylabel(\"Second PCA Dimension\")\nplt.legend()\nplt.title(\"2 Dimension PCA Visualization\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the PCA Model\nmodel_pca = PCA(n_components = 2)\ntrain_data_2dims = model_pca.fit_transform(train_data)\n\n# Build the KMeans Model\nmodel_kmeans = KMeans(n_init=10,n_clusters=6)\nkmeans_clusters = model_kmeans.fit(train_data_2dims)\n\n# Create The Graph\nfig=plt.figure(figsize=(10,10))\nplt.xlim([-3.3,3.7])\nplt.ylim([-3.3,3.7])\nplt.scatter(train_data_2dims[:,0][train_labels==1],train_data_2dims[:,1][train_labels==1],marker=\"o\",color='red', edgecolor='silver',label=\"poisonous\")\nplt.scatter(train_data_2dims[:,0][train_labels==0],train_data_2dims[:,1][train_labels==0],marker=\"o\",color='green', edgecolor='silver',label=\"non-poisonous\")\nplt.scatter(kmeans_clusters.cluster_centers_[:,0],kmeans_clusters.cluster_centers_[:,1],marker=\"o\",color='black', label=\"cluster centrioid\",s=110,alpha=0.8)\n\n#Get All Cluster Centers\nfor idx,centerxy in enumerate(kmeans_clusters.cluster_centers_):\n\n    #Set Radious to zero\n    circle_radius = 0\n\n    # Determine the distance between the center and find the largest radius\n    for xy_point in train_data_2dims[kmeans_clusters.labels_ == idx]:\n        new_circle_rad = np.linalg.norm(centerxy-xy_point)\n\n        #if new radius is bigger set it as the one we want\n        if new_circle_rad > circle_radius:\n            circle_radius = new_circle_rad\n\n    # Draw the circle\n    circle = plt.Circle(centerxy,circle_radius,alpha=0.1,edgecolor='black')        \n    plt.gca().add_artist(circle)    \n\nplt.xlabel(\"First PCA Dimension\")\nplt.ylabel(\"Second PCA Dimension\")\nplt.legend()\nplt.title(\"2 Dimension PCA Visualization\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Declare the required variable combinations\ncov_matrix_type_list = ['spherical', 'diag', 'tied', 'full']\ncomponents_list = [1,2,3,4]\n\n# Build PCA Model\nmodel_pca = PCA(n_components = 2)\ntrain_data_2dims = model_pca.fit_transform(train_data)\n\n# Only for Positive Cases\npos_ex = train_data_2dims[train_labels==1]\n\n# Declare fig subplot\nfig, ax = plt.subplots(figsize=(25,65) , sharex=True,sharey=True)\n\n# Loop for the required combinations\nfor idx, cov_matrix_type in enumerate(cov_matrix_type_list):\n    for idx2, components in enumerate(components_list):\n\n        #Create GMM Model\n        model_GMM = GaussianMixture(n_components=components,covariance_type=cov_matrix_type,random_state=12345)\n        model_GMM.fit(pos_ex)\n\n        #Make the contour \n        plt.subplot(10,4,1+(4*idx2+idx)) \n\n        # This portion of code is from the provided example https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html#example-mixture-plot-gmm-pdf-py  \n        x = np.linspace(-3.3,3.7)\n        y = np.linspace(-3.3,3.7)            \n        X, Y = np.meshgrid(x, y)\n        XX = np.array([X.ravel(), Y.ravel()]).T\n        Z = -model_GMM.score_samples(XX)\n        Z = Z.reshape(X.shape)            \n        CS = plt.contour(X, Y, Z)            \n        CB = plt.colorbar(CS, shrink=0.8)\n\n        #Plot the graph\n        plt.scatter(train_data_2dims[:,0][train_labels==1],train_data_2dims[:,1][train_labels==1],marker=\"o\",color='red')\n        plt.xlabel(\"First PCA Dimension\")\n        plt.ylabel(\"Second PCA Dimension\")\n        plt.title(\"Component = %0.0f\" % components  + \", Covariance Matrix Type = \" + str(cov_matrix_type))\n        plt.subplots_adjust(hspace=0.3,wspace=0.08)\n\n# Save the entire subplot to an image file\nplt.savefig(\"cov_plot.png\")        \n        \n#show the entire subplot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build PCA Model\nmodel_pca = PCA(n_components = 2)\ntrain_data_2dims = model_pca.fit_transform(train_data)\ntest_data_2dims = model_pca.transform(test_data)\n\n# Split data into pos and neg\npos_ex = train_data_2dims[train_labels==1]\nneg_ex = train_data_2dims[train_labels==0]\n\n#Create GMM Model for positive\nmodel_GMM_pos = GaussianMixture(n_components=4,covariance_type='full',random_state=12345)\nmodel_GMM_pos.fit(pos_ex)\n\n#Create GMM Model for negative\nmodel_GMM_neg = GaussianMixture(n_components=4,covariance_type='full',random_state=12345)\nmodel_GMM_neg.fit(neg_ex)\n\n#accuracy calculation\nhigher_prob_label_case = (model_GMM_pos.score_samples(test_data_2dims) > model_GMM_neg.score_samples(test_data_2dims)).astype(int)\ncorrect_predictions = np.where(higher_prob_label_case == np.array(test_labels),1,0)\n\nprint(\"The accuracy of the predictions on the test data is = %0.02f %%\" % ((correct_predictions.sum()/test_labels.size)*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cov_matrix_type_list = ['spherical', 'diag', 'tied', 'full']\npca_components_list = range(1,100,1)\ngmm_components_list = range(1,100,1)\nresult = []\nhighest_accuracy = 0\n\nprint(\"Running experiments:\")\n\n# For all the possible combinations of pca/gmm and cov_matrix_type\nfor cov in cov_matrix_type_list:\n    for pca in pca_components_list:\n        for gmm in gmm_components_list:\n\n            # Based on the provided parameter count definition\n            if cov == 'tied':\n                param_count = 2 * ((pca * gmm) + (pca*(pca+1)/2 ))\n            elif cov == 'spherical':\n                param_count = 2 * (gmm * pca + gmm)\n            elif cov == 'diag':\n                param_count = 4 * gmm * pca\n            elif cov == 'full':\n                param_count = 2 * ((pca * gmm) + (pca*(pca+1)/2 * gmm ))\n\n            # Only calculate if the parameter count is <= 50\n            if param_count <= 50:\n\n                # Build the PCA Model\n                model_pca = PCA(n_components = pca)\n                train_data_trans = model_pca.fit_transform(train_data)\n                test_data_trans = model_pca.transform(test_data)\n\n                # Create positive and negative cases\n                pos_ex = train_data_trans[train_labels==1]\n                neg_ex = train_data_trans[train_labels==0]\n\n                # Build the GMM model for positive \n                model_GMM_pos = GaussianMixture(n_components=gmm, covariance_type=cov, random_state=12345) \n                model_GMM_pos.fit(pos_ex)\n\n                # Build the GMM model for negative\n                model_GMM_neg = GaussianMixture(n_components=gmm, covariance_type=cov, random_state=12345) \n                model_GMM_neg.fit(neg_ex)\n\n                #accuracy calculation, taking the better of pos/neg\n                higher_prob_label_case = (model_GMM_pos.score_samples(test_data_trans) > model_GMM_neg.score_samples(test_data_trans)).astype(int)\n                correct_predictions = np.where(higher_prob_label_case == np.array(test_labels),1,0)\n                accuracy = ((correct_predictions.sum()/test_labels.size)*100)\n\n                # Records the highest accuracy configuration\n                if accuracy > highest_accuracy:\n                    result.append({'cov':cov,'pca':pca,'gmm':gmm,'param_count':param_count,'accuracy':accuracy})\n                    highest_accuracy = accuracy\n                print(\"Covariance = \" + cov + \" | PCA Components = %0.f\" % pca + \" | GMM Components = %0.f \" % gmm + \" | Parameters Count = %0.0f\" % param_count + \" | Accuracy = %0.2f %%\" %accuracy)\n\nprint (\"=====================================================================================================================\")\nprint(\"Gaussian mixture model that results in the best accuracy with no more than 50 parameters is:\")    \ncov = result[(len(result)-1)]['cov']\npca = result[(len(result)-1)]['pca']\ngmm = result[(len(result)-1)]['gmm']\nparam_count = result[(len(result)-1)]['param_count']\naccuracy = result[(len(result)-1)]['accuracy']\nprint(\"Covariance = \" + cov + \" | PCA Components = %0.f\" % pca + \" | GMM Components = %0.f \" % gmm + \" | Parameters Count = %0.0f\" % param_count + \" | Accuracy = %0.2f %%\" %accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}